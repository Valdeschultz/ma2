{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI & Machine Learning (KAN-CINTO4003U) - Copenhagen Business School | Spring 2025**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part II: BERT\n",
    "\n",
    "Please see the description of the assignment in the README file (section 2) <br>\n",
    "**Guide notebook**: [guides/bert_guide.ipynb](guides/bert_guide.ipynb)\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "<br>\n",
    "\n",
    "* Note that you should report results using a classification report. \n",
    "\n",
    "* Also, remember to include some reflections on your results: how do they compare with the results from Part I, BoW? Are there any hyperparameters that are particularly important?\n",
    "\n",
    "* You should follow the steps given in the `bert_guide` notebook\n",
    "\n",
    "* **Optionally**, you can fine-tune a pre-trained BERT model to classify news articles as is done in [guides/bert_guide_finetuning.ipybb](guides/bert_guide_finetuning.ipybb), the same task as in part 1. As this requires more computational resources, this part is optional. If you do decide to complete this part, you will need to use a GPU (e.g., Google Colab) to train the model. (For reference, training on a 2020 Macbook Pro with 16GB RAM and a M1 chip results in an out-of-memory error). Therefore, we suggest that you use Google Colab or another cloud-based service with a GPU. You can easily upload the `bert_guide_finetuning.ipynb` notebook to Google Colab and run it there.\n",
    "\n",
    "<br>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vald0\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports for the project\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from datasets import load_dataset, DatasetDict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data\n",
    "\n",
    "We can load this data directly from [Hugging Face Datasets](https://huggingface.co/docs/datasets/) - The HuggingFace Hub- into a Pandas DataFrame. Pretty neat!\n",
    "\n",
    "**Note**: This cell will download the dataset and keep it in memory. If you run this cell multiple times, it will download the dataset multiple times.\n",
    "\n",
    "You are welcome to increase the `frac` parameter to load more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have to make sets smaller due to HUGE processing times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since fancyzhx/ag_news couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at C:\\Users\\vald0\\.cache\\huggingface\\datasets\\fancyzhx___ag_news\\default\\0.0.0\\eb185aade064a813bc0b7f42de02595523103ca4 (last modified on Sun Mar 30 13:41:37 2025).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 12000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#making the datasets smaller due to large prosessing time\n",
    "ag_news_train = load_dataset(\"fancyzhx/ag_news\", split=\"train[:10%]\", keep_in_memory=True)  # 20% of the training data\n",
    "ag_news_test = load_dataset(\"fancyzhx/ag_news\", split=\"test[:10%]\", keep_in_memory=True)  # 20% of the test data\n",
    "\n",
    "ag_news = DatasetDict({\n",
    "    \"train\": ag_news_train,\n",
    "    \"test\": ag_news_test\n",
    "})\n",
    "\n",
    "ag_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Load the tokenizer and model\n",
    "embedder = pipeline(\n",
    "    model=\"answerdotai/ModernBERT-base\",      # model used for embedding\n",
    "    tokenizer=\"answerdotai/ModernBERT-base\",  # tokenizer used for embedding\n",
    "    task=\"feature-extraction\",                # feature extraction task (returns embeddings)\n",
    "    device= 0                                  # use GPU 0 if available\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 12000/12000 [16:53<00:00, 11.84 examples/s]\n",
      "Map: 100%|██████████| 760/760 [01:06<00:00, 11.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def get_embeddings(data):\n",
    "    \"\"\" Extract the [CLS] embedding for each text. \"\"\"\n",
    "    embeddings = embedder(data[\"text\"])  # Full token embeddings\n",
    "    cls_embeddings = [e[0][0] for e in embeddings]  # Extract first token ([CLS])\n",
    "    return {\"embeddings\": cls_embeddings}\n",
    "\n",
    "ag_news = ag_news.map(get_embeddings, batched=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show embeddings as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'embeddings'],\n",
       "        num_rows: 12000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'embeddings'],\n",
       "        num_rows: 760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract features and labels into our traning and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (12000, 768), y_train shape: (12000,)\n",
      "X_test shape: (760, 768), y_test shape: (760,)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(ag_news[\"train\"][\"embeddings\"])  # Feature embeddings\n",
    "y_train = np.array(ag_news[\"train\"][\"label\"])       # Labels\n",
    "\n",
    "X_test = np.array(ag_news[\"test\"][\"embeddings\"])\n",
    "y_test = np.array(ag_news[\"test\"][\"label\"])\n",
    "\n",
    "# Check shapes\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93      2976\n",
      "           1       0.97      0.99      0.98      2789\n",
      "           2       0.89      0.88      0.88      3039\n",
      "           3       0.90      0.90      0.90      3196\n",
      "\n",
      "    accuracy                           0.92     12000\n",
      "   macro avg       0.92      0.92      0.92     12000\n",
      "weighted avg       0.92      0.92      0.92     12000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vald0\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=5000)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = lr.predict(X_train)\n",
    "\n",
    "print(classification_report(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.87       197\n",
      "           1       0.95      0.94      0.95       199\n",
      "           2       0.81      0.84      0.83       158\n",
      "           3       0.88      0.85      0.86       206\n",
      "\n",
      "    accuracy                           0.88       760\n",
      "   macro avg       0.88      0.88      0.88       760\n",
      "weighted avg       0.88      0.88      0.88       760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##hyperparameter tuning with GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],  # Regularization type\n",
    "    'solver': ['liblinear', 'saga'],  # Solver for logistic regression\n",
    "    'max_iter': [1000, 5000, 7000],  # Number of iterations for convergence\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(lr, param_grid, cv=5, scoring='f1')\n",
    "grid.fit(X_train, y_train)\n",
    "# Print the best hyperparameters and score\n",
    "print(\"Best hyperparameters:\", grid.best_params_)\n",
    "print(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))\n",
    "\n",
    "# Use the best estimator to predict and evaluate on the test set\n",
    "best_lr = grid.best_estimator_\n",
    "y_pred_grid = best_lr.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second classifier prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.80      0.80       197\n",
      "           1       0.88      0.91      0.90       199\n",
      "           2       0.68      0.73      0.70       158\n",
      "           3       0.80      0.72      0.76       206\n",
      "\n",
      "    accuracy                           0.79       760\n",
      "   macro avg       0.79      0.79      0.79       760\n",
      "weighted avg       0.80      0.79      0.79       760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import randomforrest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter grid for RandomForestClassifier\n",
    "param_grid_rf = {\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'n_estimators': [50, 100, 200]\n",
    "}\n",
    "\n",
    "grid_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=5, scoring='f1')\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best hyperparameters:\", grid_rf.best_params_)\n",
    "print(\"Best cross-validation accuracy: {:.2f}\".format(grid_rf.best_score_))\n",
    "\n",
    "# Evaluate the best estimator on the test set\n",
    "best_rf = grid_rf.best_estimator_\n",
    "y_pred_rf_grid = best_rf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_rf_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reflections\n",
    "Only experimented with different C values for the logisticregression, computation could not finish if gridsearch had too many variables\n",
    "\n",
    "Logisticregression:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.87      0.88      0.87       197\n",
    "           1       0.95      0.94      0.95       199\n",
    "           2       0.81      0.84      0.83       158\n",
    "           3       0.88      0.85      0.86       206\n",
    "\n",
    "    accuracy                           0.88       760\n",
    "   macro avg       0.88      0.88      0.88       760\n",
    "weighted avg       0.88      0.88      0.88       760\n",
    "\n",
    "\n",
    "\n",
    "For random forrest:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.79      0.80      0.80       197\n",
    "           1       0.88      0.91      0.90       199\n",
    "           2       0.68      0.73      0.70       158\n",
    "           3       0.80      0.72      0.76       206\n",
    "\n",
    "    accuracy                           0.79       760\n",
    "   macro avg       0.79      0.79      0.79       760\n",
    "weighted avg       0.80      0.79      0.79       760\n",
    "\n",
    "Initially logisticregression scored better, and that was without scaling the data. \n",
    "\n",
    "The bert model requires huge processseing times and power"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
